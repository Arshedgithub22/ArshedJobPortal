{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arshedgithub22/ArshedJobPortal/blob/master/LivePortrait_Optimization_Code_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEQ9anwsI1HW",
        "outputId": "0f7f3692-b0da-4e67-f3ce-068264abe399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (4.25.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m123.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "## LivePortrait Code Optimization Examples\n",
        "\n",
        "This Colab notebook provides runnable examples for the proposed optimization techniques\n",
        "for the LivePortrait model, focusing on reducing inference time and GPU memory usage.\n",
        "\n",
        "**NOTE:** This code uses a *generic* PyTorch model as an example. You will need to\n",
        "adapt the `model` loading, `input_data` preparation, and the actual `inference_function`\n",
        "calls to match the specific implementation details of the LivePortrait GitHub repository.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Environment Setup\n",
        "\n",
        "First, let's set up the Colab environment by cloning the (placeholder) LivePortrait repository\n",
        "and installing necessary dependencies. You'll need to replace `https://github.com/YourLivePortraitRepo/LivePortrait.git`\n",
        "con el URL actual.\n",
        "\n",
        "**IMPORTANT:** Asegúrate de que todos los comandos `!pip install` se ejecuten correctamente en la primera celda\n",
        "de tu notebook de Colab para evitar errores `ModuleNotFoundError`.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# --- Step 1.1: Install necessary Python packages ---\n",
        "# These installations MUST happen before the imports below to ensure modules are available.\n",
        "print(\"--- Installing required Python packages ---\")\n",
        "!pip install numpy torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Or appropriate CUDA version\n",
        "!pip install onnxruntime # THIS IS CRUCIAL FOR THE ONNXRUNTIME ERROR\n",
        "# If LivePortrait has a requirements.txt, you would use:\n",
        "# !pip install -r requirements.txt\n",
        "print(\"--- Package installation complete ---\")\n",
        "\n",
        "\n",
        "# --- Step 1.2: Import necessary libraries ---\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda.amp import autocast # For Mixed Precision (FP16)\n",
        "import onnxruntime # For ONNX model inference\n",
        "import numpy as np\n",
        "import os\n",
        "import gc # For garbage collection and clearing CUDA cache\n",
        "\n",
        "\n",
        "# --- Step 1.3: Placeholder for LivePortrait specific imports and functions ---\n",
        "# In a real scenario, you would replace these with actual LivePortrait imports\n",
        "# from live_portrait.models.model_builder import LivePortraitModel\n",
        "# from live_portrait.utils.io import load_checkpoint\n",
        "# from live_portrait.data.data_loader import get_sample_data\n",
        "# from live_portrait.inference.core import inference_live_portrait\n",
        "\n",
        "\n",
        "# A simple placeholder model for demonstration purposes, mimicking a deep learning model\n",
        "# You will replace this with the actual LivePortrait model definition/loading.\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Linear(256, 1000) # Example output features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Placeholder for LivePortrait's model loading and inference logic\n",
        "# YOU MUST REPLACE THE CONTENT OF THESE FUNCTIONS WITH THE ACTUAL LIVEPORTRAIT CODE\n",
        "def load_liveportrait_model_conceptual():\n",
        "    \"\"\"\n",
        "    Conceptual function to load the LivePortrait model.\n",
        "    In a real scenario, this would load the actual LivePortrait model from a checkpoint.\n",
        "    \"\"\"\n",
        "    print(\"Loading conceptual LivePortrait model (replace with actual LivePortrait model loading)...\")\n",
        "    # --- REPLACE BELOW WITH ACTUAL LIVEPORTRAIT MODEL LOADING LOGIC ---\n",
        "    # Example for LivePortrait:\n",
        "    # from liveportrait.models.builder import build_model\n",
        "    # from liveportrait.utils.checkpoint import load_checkpoint\n",
        "    # config = ... # Load LivePortrait config\n",
        "    # model = build_model(config.model_params)\n",
        "    # load_checkpoint(model, 'path/to/liveportrait_checkpoint.pth') # Ensure this path is correct\n",
        "    # --- END REPLACE ---\n",
        "\n",
        "    # Placeholder model for demonstration (delete this once you have real LivePortrait code)\n",
        "    model = SimpleCNN()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "    model.eval() # Set to evaluation mode for inference\n",
        "    return model\n",
        "\n",
        "def prepare_liveportrait_input_conceptual(batch_size=1):\n",
        "    \"\"\"\n",
        "    Conceptual function to prepare LivePortrait input data.\n",
        "    In a real scenario, this would load and preprocess source image and driving video.\n",
        "    Returns a dummy tensor for demonstration.\n",
        "    \"\"\"\n",
        "    print(f\"Preparing conceptual LivePortrait input for batch size {batch_size} (replace with actual LivePortrait data loading)...\")\n",
        "    # --- REPLACE BELOW WITH ACTUAL LIVEPORTRAIT INPUT PREPARATION LOGIC ---\n",
        "    # Example for LivePortrait:\n",
        "    # source_image_path = 'path/to/your/source_image.jpg'\n",
        "    # driving_video_path = 'path/to/your/driving_video.mp4'\n",
        "    # # Load and preprocess as per LivePortrait's data pipeline\n",
        "    # source_image = ... # Load source image\n",
        "    # driving_frames = ... # Load driving video frames\n",
        "    # preprocessed_inputs = preprocess_for_liveportrait(source_image, driving_frames)\n",
        "    # This might result in multiple tensors or a dictionary of tensors.\n",
        "    # For this example, we'll return a single tensor.\n",
        "    # --- END REPLACE ---\n",
        "\n",
        "    # Placeholder dummy input for demonstration (delete this once you have real LivePortrait code)\n",
        "    dummy_input = torch.randn(batch_size, 3, 256, 256) # Example: Batch, Channels, Height, Width\n",
        "    if torch.cuda.is_available():\n",
        "        dummy_input = dummy_input.cuda()\n",
        "    return dummy_input\n",
        "\n",
        "def run_liveportrait_inference_conceptual(model, input_data):\n",
        "    \"\"\"\n",
        "    Conceptual function to run LivePortrait inference.\n",
        "    In a real scenario, this would invoke the core LivePortrait inference logic.\n",
        "    Returns a dummy output tensor.\n",
        "    \"\"\"\n",
        "    print(\"Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\")\n",
        "    # Disable gradient computation for inference\n",
        "    with torch.no_grad():\n",
        "        # --- REPLACE BELOW WITH ACTUAL LIVEPORTRAIT INFERENCE CALL ---\n",
        "        # Example for LivePortrait:\n",
        "        # result_frames = inference_live_portrait(model, input_data['source'], input_data['driving'])\n",
        "        # --- END REPLACE ---\n",
        "\n",
        "        # Placeholder output for demonstration (delete this once you have real LivePortrait code)\n",
        "        output = model(input_data)\n",
        "    return output\n",
        "\n",
        "def clear_cuda_cache():\n",
        "    \"\"\"Clears CUDA cache to get more accurate memory measurements.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "print(\"\\n--- Initializing LivePortrait Repository (Conceptual) ---\")\n",
        "\n",
        "# Replace with the actual GitHub repository URL\n",
        "liveportrait_repo_url = \"https://github.com/KwaiVGI/LivePortrait.git\" # <--- UPDATED TO THE ACTUAL LIVEPORTRAIT GITHUB URL\n",
        "liveportrait_repo_dir = \"LivePortrait\" # Assuming the cloned directory name\n",
        "\n",
        "# Only clone if not already cloned (for re-running cells)\n",
        "if not os.path.exists(liveportrait_repo_dir):\n",
        "    print(f\"Cloning {liveportrait_repo_url}...\")\n",
        "    # Execute the git clone command\n",
        "    !git clone {liveportrait_repo_url}\n",
        "\n",
        "    # IMPORTANT CHECK: Verify if the directory was created after cloning\n",
        "    if not os.path.exists(liveportrait_repo_dir):\n",
        "        print(f\"\\nERROR: Directory '{liveportrait_repo_dir}' was not created after cloning '{liveportrait_repo_url}'.\")\n",
        "        print(\"Please check the following:\")\n",
        "        print(\"1. Is the 'liveportrait_repo_url' correct and publicly accessible?\")\n",
        "        print(\"2. Does the repository actually clone into a folder named 'LivePortrait'? It might be a different name.\")\n",
        "        print(\"   If it clones into a different name (e.g., 'live_portrait_repo'), update 'liveportrait_repo_dir' accordingly.\")\n",
        "        raise FileNotFoundError(f\"Cloned directory '{liveportrait_repo_dir}' not found. Aborting setup.\")\n",
        "\n",
        "    # Change current directory into the cloned repository\n",
        "    # IMPORTANT: After cloning, all subsequent file paths should be relative to this directory\n",
        "    print(f\"Changing directory to {liveportrait_repo_dir}\")\n",
        "    os.chdir(liveportrait_repo_dir)\n",
        "\n",
        "    # Install specific requirements for LivePortrait if not covered above\n",
        "    if os.path.exists('requirements.txt'):\n",
        "        print(\"Installing LivePortrait specific requirements from requirements.txt...\")\n",
        "        !pip install -r requirements.txt\n",
        "    else:\n",
        "        print(\"No requirements.txt found in LivePortrait repo. Ensure all dependencies are met.\")\n",
        "\n",
        "    # Placeholder for downloading pre-trained models\n",
        "    # YOU WILL LIKELY NEED TO DOWNLOAD LIVEPORTRAIT PRE-TRAINED MODELS HERE\n",
        "    # Check LivePortrait's GitHub README for instructions on downloading models.\n",
        "    # Example:\n",
        "    # if not os.path.exists(\"checkpoints\"):\n",
        "    #     os.makedirs(\"checkpoints\")\n",
        "    #     print(\"Created 'checkpoints' directory.\")\n",
        "    # !wget -O checkpoints/model_name.pth https://example.com/path/to/pretrained/liveportrait_model.pth\n",
        "\n",
        "else:\n",
        "    print(f\"Directory '{liveportrait_repo_dir}' already exists. Skipping cloning.\")\n",
        "    # Ensure we are in the correct directory if rerunning\n",
        "    if os.getcwd().split('/')[-1] != liveportrait_repo_dir:\n",
        "        print(f\"Changing directory to {liveportrait_repo_dir}\")\n",
        "        os.chdir(liveportrait_repo_dir)\n",
        "\n",
        "\n",
        "print(\"\\nEnvironment setup complete (conceptual LivePortrait repo setup).\\n\")\n",
        "\n",
        "\"\"\"\n",
        "### 2. Original Implementation and Baseline Measurement\n",
        "\n",
        "We'll establish a baseline for inference time and GPU memory usage using the original\n",
        "(conceptual) model and inference function.\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- Running Original Implementation (Baseline) ---\")\n",
        "\n",
        "clear_cuda_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# Load the conceptual LivePortrait model\n",
        "model_original = load_liveportrait_model_conceptual()\n",
        "# Prepare a single input for latency measurement\n",
        "input_original = prepare_liveportrait_input_conceptual(batch_size=1)\n",
        "\n",
        "# Warm-up run: The first run can be slower due to CUDA context initialization\n",
        "# Run a few times to get stable measurements\n",
        "print(\"Running warm-up for original model...\")\n",
        "for _ in range(5):\n",
        "    _ = run_liveportrait_inference_conceptual(model_original, input_original)\n",
        "clear_cuda_cache() # Clear cache after warm-up\n",
        "\n",
        "# Actual timed run\n",
        "print(\"Starting actual timed run for original model...\")\n",
        "start_time_original = time.time()\n",
        "# Run inference with the original model\n",
        "output_original = run_liveportrait_inference_conceptual(model_original, input_original)\n",
        "end_time_original = time.time()\n",
        "\n",
        "inference_time_original = end_time_original - start_time_original\n",
        "print(f\"\\nOriginal Inference Time: {inference_time_original:.4f} seconds\")\n",
        "\n",
        "max_memory_original = 0\n",
        "if torch.cuda.is_available():\n",
        "    max_memory_original = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "    print(f\"Original Max GPU Memory Usage: {max_memory_original:.2f} GB\")\n",
        "else:\n",
        "    print(\"CUDA not available, GPU memory usage not measured.\")\n",
        "\n",
        "print(\"Original Output Shape (conceptual):\", output_original.shape)\n",
        "# In a real LivePortrait scenario, you'd save or display the output video/image here.\n",
        "# Example: save_video(output_original, 'results/output_original.mp4')\n",
        "\n",
        "# Clean up original model to free memory before optimizations\n",
        "del model_original\n",
        "del input_original\n",
        "del output_original\n",
        "clear_cuda_cache()\n",
        "\n",
        "\"\"\"\n",
        "### 3. Optimized Implementations\n",
        "\n",
        "Now, let's apply and test each of the proposed optimization techniques.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3.1. Mixed Precision (FP16) Inference\n",
        "\n",
        "This involves using `torch.cuda.amp.autocast` to perform operations in half-precision.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Running Optimized Implementation: Mixed Precision (FP16) ---\")\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Skipping FP16 optimization: CUDA not available.\")\n",
        "    # Placeholder values for summary table if skipped\n",
        "    inference_time_fp16 = \"N/A (CUDA)\"\n",
        "    max_memory_fp16 = \"N/A (CUDA)\"\n",
        "else:\n",
        "    clear_cuda_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    model_fp16 = load_liveportrait_model_conceptual() # Load model again\n",
        "    # Ensure model is on GPU for FP16\n",
        "    model_fp16 = model_fp16.cuda() # Already done in load_... if CUDA available\n",
        "\n",
        "    input_fp16 = prepare_liveportrait_input_conceptual(batch_size=1)\n",
        "    # Ensure input is on GPU for FP16\n",
        "    input_fp16 = input_fp16.cuda() # Already done in prepare_... if CUDA available\n",
        "\n",
        "    # Warm-up run\n",
        "    print(\"Running warm-up for FP16 model...\")\n",
        "    for _ in range(5):\n",
        "        with autocast():\n",
        "            _ = run_liveportrait_inference_conceptual(model_fp16, input_fp16)\n",
        "    clear_cuda_cache() # Clear cache after warm-up\n",
        "\n",
        "    # Actual timed run\n",
        "    print(\"Starting actual timed run for FP16 model...\")\n",
        "    start_time_fp16 = time.time()\n",
        "    with autocast(): # Apply autocast context manager for FP16\n",
        "        output_fp16 = run_liveportrait_inference_conceptual(model_fp16, input_fp16)\n",
        "    end_time_fp16 = time.time()\n",
        "\n",
        "    inference_time_fp16 = end_time_fp16 - start_time_fp16\n",
        "    print(f\"\\nFP16 Inference Time: {inference_time_fp16:.4f} seconds\")\n",
        "\n",
        "    max_memory_fp16 = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "    print(f\"FP16 Max GPU Memory Usage: {max_memory_fp16:.2f} GB\")\n",
        "    print(\"FP16 Output Shape (conceptual):\", output_fp16.shape)\n",
        "\n",
        "    del model_fp16\n",
        "    del input_fp16\n",
        "    del output_fp16\n",
        "    clear_cuda_cache()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "#### 3.2. Model Export to ONNX and Inference with ONNX Runtime\n",
        "\n",
        "This involves exporting the PyTorch model to ONNX format and then running inference\n",
        "using the ONNX Runtime.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Running Optimized Implementation: ONNX Runtime ---\")\n",
        "\n",
        "# Helper function to convert PyTorch tensor to NumPy array\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "onnx_model_path = \"liveportrait_optimized.onnx\"\n",
        "\n",
        "try:\n",
        "    # Load the conceptual LivePortrait model for ONNX export\n",
        "    model_onnx_export = load_liveportrait_model_conceptual()\n",
        "    # Create a dummy input for tracing the model\n",
        "    # IMPORTANT: The dummy input must have the EXACT same shape and dtype as your real input\n",
        "    # for ONNX tracing to be successful.\n",
        "    dummy_input_onnx = prepare_liveportrait_input_conceptual(batch_size=1)\n",
        "\n",
        "    print(f\"Exporting model to ONNX: {onnx_model_path}\")\n",
        "    torch.onnx.export(model_onnx_export,\n",
        "                      dummy_input_onnx,\n",
        "                      onnx_model_path,\n",
        "                      export_params=True,\n",
        "                      opset_version=17, # Recommended opset version for modern PyTorch\n",
        "                      do_constant_folding=True,\n",
        "                      input_names=['input'],\n",
        "                      output_names=['output'],\n",
        "                      dynamic_axes={'input' : {0 : 'batch_size'}} # Allow dynamic batch size for flexible inputs\n",
        "                     )\n",
        "    print(\"Model exported to ONNX successfully.\")\n",
        "\n",
        "    # Determine providers for ONNX Runtime\n",
        "    # Prefer CUDAExecutionProvider if available, otherwise fallback to CPU\n",
        "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']\n",
        "    print(f\"ONNX Runtime providers: {providers}\")\n",
        "\n",
        "    # Load the ONNX model and create an ONNX Runtime session\n",
        "    ort_session = onnxruntime.InferenceSession(onnx_model_path, providers=providers)\n",
        "    print(\"ONNX Runtime session created.\")\n",
        "\n",
        "    clear_cuda_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    # Prepare input for ONNX Runtime (needs to be NumPy array on CPU)\n",
        "    input_onnx_np = to_numpy(prepare_liveportrait_input_conceptual(batch_size=1))\n",
        "\n",
        "    # Warm-up run for ONNX Runtime\n",
        "    print(\"Running warm-up for ONNX Runtime model...\")\n",
        "    for _ in range(5):\n",
        "        ort_inputs = {ort_session.get_inputs()[0].name: input_onnx_np}\n",
        "        _ = ort_session.run(None, ort_inputs)\n",
        "    clear_cuda_cache() # Clear cache after warm-up\n",
        "\n",
        "    # Actual timed run\n",
        "    print(\"Starting actual timed run for ONNX Runtime model...\")\n",
        "    start_time_onnx = time.time()\n",
        "    # Run inference with ONNX Runtime\n",
        "    ort_inputs = {ort_session.get_inputs()[0].name: input_onnx_np}\n",
        "    ort_outs = ort_session.run(None, ort_inputs) # ort_outs is a list of output arrays\n",
        "    output_onnx = ort_outs[0] # Assuming single output\n",
        "    end_time_onnx = time.time()\n",
        "\n",
        "    inference_time_onnx = end_time_onnx - start_time_onnx\n",
        "    print(f\"\\nONNX Runtime Inference Time: {inference_time_onnx:.4f} seconds\")\n",
        "\n",
        "    # ONNX Runtime memory usage might not be directly captured by torch.cuda.max_memory_allocated()\n",
        "    # as it manages its own memory. For a comprehensive comparison, you might need NVIDIA-SMI.\n",
        "    max_memory_onnx = 0\n",
        "    if torch.cuda.is_available():\n",
        "        max_memory_onnx = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "        print(f\"ONNX Runtime Max GPU Memory Usage (PyTorch perspective): {max_memory_onnx:.2f} GB\")\n",
        "        print(\"Note: Actual ONNX Runtime memory usage might differ and may require external tools like `nvidia-smi` to measure.\")\n",
        "\n",
        "    print(\"ONNX Runtime Output Shape (conceptual):\", output_onnx.shape)\n",
        "\n",
        "    del model_onnx_export\n",
        "    del dummy_input_onnx\n",
        "    del ort_session # Delete the session\n",
        "    clear_cuda_cache()\n",
        "    os.remove(onnx_model_path) # Clean up the exported ONNX model file\n",
        "    print(f\"Cleaned up {onnx_model_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ONNX Export/Inference failed: {e}\")\n",
        "    # Ensure cleanup even if error occurs\n",
        "    if os.path.exists(onnx_model_path):\n",
        "        os.remove(onnx_model_path)\n",
        "    # Placeholder values for summary table if skipped\n",
        "    inference_time_onnx = \"N/A (ONNX Fail)\"\n",
        "    max_memory_onnx = \"N/A (ONNX Fail)\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "#### 3.3. JIT Compilation with `torch.compile` (PyTorch 2.0+)\n",
        "\n",
        "This optimization leverages PyTorch 2.0's `torch.compile` feature for graph compilation.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Running Optimized Implementation: torch.compile ---\")\n",
        "\n",
        "# Check PyTorch version\n",
        "if not torch.__version__.startswith(\"2.\"): # Use .startswith(\"2.\") for any 2.x version\n",
        "    print(f\"Skipping torch.compile: Requires PyTorch 2.0+ (current version: {torch.__version__}).\")\n",
        "    inference_time_compiled = \"N/A (PyTorch < 2.0)\"\n",
        "    max_memory_compiled = \"N/A (PyTorch < 2.0)\"\n",
        "elif not torch.cuda.is_available():\n",
        "    print(\"Skipping torch.compile: CUDA not available, torch.compile is most effective on GPU.\")\n",
        "    inference_time_compiled = \"N/A (CUDA)\"\n",
        "    max_memory_compiled = \"N/A (CUDA)\"\n",
        "else:\n",
        "    clear_cuda_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    model_compiled = load_liveportrait_model_conceptual() # Load model again\n",
        "    # Ensure model is on GPU for compilation\n",
        "    model_compiled = model_compiled.cuda()\n",
        "\n",
        "    print(\"Compiling model with torch.compile...\")\n",
        "    # Use \"reduce-overhead\" for faster compilation, \"max-autotune\" for best performance\n",
        "    # For a real project, you might experiment with different modes.\n",
        "    compiled_model = torch.compile(model_compiled, mode=\"reduce-overhead\")\n",
        "    print(\"Model compiled.\")\n",
        "\n",
        "    input_compiled = prepare_liveportrait_input_conceptual(batch_size=1)\n",
        "    # Ensure input is on GPU\n",
        "    input_compiled = input_compiled.cuda()\n",
        "\n",
        "    # Warm-up run for compiled model\n",
        "    print(\"Running warm-up for compiled model...\")\n",
        "    for _ in range(5):\n",
        "        _ = run_liveportrait_inference_conceptual(compiled_model, input_compiled)\n",
        "    clear_cuda_cache() # Clear cache after warm-up\n",
        "\n",
        "    # Actual timed run\n",
        "    print(\"Starting actual timed run for compiled model...\")\n",
        "    start_time_compiled = time.time()\n",
        "    output_compiled = run_liveportrait_inference_conceptual(compiled_model, input_compiled)\n",
        "    end_time_compiled = time.time()\n",
        "\n",
        "    inference_time_compiled = end_time_compiled - start_time_compiled\n",
        "    print(f\"\\ntorch.compile Inference Time: {inference_time_compiled:.4f} seconds\")\n",
        "\n",
        "    max_memory_compiled = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "    print(f\"torch.compile Max GPU Memory Usage: {max_memory_compiled:.2f} GB\")\n",
        "    print(\"torch.compile Output Shape (conceptual):\", output_compiled.shape)\n",
        "\n",
        "    del model_compiled\n",
        "    del compiled_model\n",
        "    del input_compiled\n",
        "    del output_compiled\n",
        "    clear_cuda_cache()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "### 4. Consolidated Performance Summary (Conceptual)\n",
        "\n",
        "This section would present the combined results from the actual runs.\n",
        "You would fill in the table with the *measured* values.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Consolidated Performance Summary (Fill with Actual Measured Values) ---\")\n",
        "\n",
        "# Retrieve actual measured values from the execution above or use N/A if skipped\n",
        "actual_inference_time_original = inference_time_original if 'inference_time_original' in locals() else \"N/A (Error)\"\n",
        "actual_max_memory_original = max_memory_original if 'max_memory_original' in locals() else \"N/A (Error)\"\n",
        "\n",
        "# Use variables defined in the respective optimization blocks\n",
        "# They will be \"N/A\" strings if the optimization was skipped\n",
        "actual_inference_time_fp16 = inference_time_fp16\n",
        "actual_max_memory_fp16 = max_memory_fp16\n",
        "\n",
        "actual_inference_time_onnx = inference_time_onnx\n",
        "actual_max_memory_onnx = max_memory_onnx\n",
        "\n",
        "actual_inference_time_compiled = inference_time_compiled\n",
        "actual_max_memory_compiled = max_memory_compiled\n",
        "\n",
        "# Ensure numerical values are formatted correctly, N/A strings handled\n",
        "def format_metric(value, decimal_places):\n",
        "    try:\n",
        "        return f\"{float(value):.{decimal_places}f}\"\n",
        "    except (ValueError, TypeError):\n",
        "        return str(value)\n",
        "\n",
        "print(f\"\\n{'Metric':<25} | {'Original':<15} | {'FP16':<15} | {'ONNX Runtime':<15} | {'torch.compile':<15}\")\n",
        "print(f\"{'-'*25}-+-{'-'*15}-+-{'-'*15}-+-{'-'*15}-+-{'-'*15}\")\n",
        "print(f\"{'Inference Time (s)':<25} | {format_metric(actual_inference_time_original, 4):<15} | {format_metric(actual_inference_time_fp16, 4):<15} | {format_metric(actual_inference_time_onnx, 4):<15} | {format_metric(actual_inference_time_compiled, 4):<15}\")\n",
        "print(f\"{'Max GPU Memory (GB)':<25} | {format_metric(actual_max_memory_original, 2):<15} | {format_metric(actual_max_memory_fp16, 2):<15} | {format_metric(actual_max_memory_onnx, 2):<15} | {format_metric(actual_max_memory_compiled, 2):<15}\")\n",
        "print(f\"{'Output Quality (Visual)':<25} | {'Baseline':<15} | {'Similar':<15} | {'Similar':<15} | {'Similar':<15}\")\n",
        "\n",
        "\"\"\"\n",
        "**Note:** The 'N/A' values indicate that the corresponding optimization could not be tested\n",
        "due to lack of CUDA support or PyTorch version incompatibility. You would replace these\n",
        "con tus valores medidos reales.\n",
        "\n",
        "Según los resultados medidos, seleccionarías la optimización o combinación\n",
        "de optimizaciones con mejor rendimiento para tu envío final, y actualizarías la sección de resumen (`liveportrait_optimization_plan`)\n",
        "en consecuencia.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w5u72U1JLPDk",
        "outputId": "1335a7c0-8939-453b-cc44-d76e801e2fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Installing required Python packages ---\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (1.22.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (4.25.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "--- Package installation complete ---\n",
            "\n",
            "--- Initializing LivePortrait Repository (Conceptual) ---\n",
            "Directory 'LivePortrait' already exists. Skipping cloning.\n",
            "Changing directory to LivePortrait\n",
            "\n",
            "Environment setup complete (conceptual LivePortrait repo setup).\n",
            "\n",
            "--- Running Original Implementation (Baseline) ---\n",
            "Loading conceptual LivePortrait model (replace with actual LivePortrait model loading)...\n",
            "Preparing conceptual LivePortrait input for batch size 1 (replace with actual LivePortrait data loading)...\n",
            "Running warm-up for original model...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Starting actual timed run for original model...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "\n",
            "Original Inference Time: 0.0020 seconds\n",
            "Original Max GPU Memory Usage: 0.04 GB\n",
            "Original Output Shape (conceptual): torch.Size([1, 1000])\n",
            "\n",
            "--- Running Optimized Implementation: Mixed Precision (FP16) ---\n",
            "Loading conceptual LivePortrait model (replace with actual LivePortrait model loading)...\n",
            "Preparing conceptual LivePortrait input for batch size 1 (replace with actual LivePortrait data loading)...\n",
            "Running warm-up for FP16 model...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-f292028e60fe>:291: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting actual timed run for FP16 model...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "\n",
            "FP16 Inference Time: 0.0025 seconds\n",
            "FP16 Max GPU Memory Usage: 0.03 GB\n",
            "FP16 Output Shape (conceptual): torch.Size([1, 1000])\n",
            "\n",
            "--- Running Optimized Implementation: ONNX Runtime ---\n",
            "Loading conceptual LivePortrait model (replace with actual LivePortrait model loading)...\n",
            "Preparing conceptual LivePortrait input for batch size 1 (replace with actual LivePortrait data loading)...\n",
            "Exporting model to ONNX: liveportrait_optimized.onnx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-f292028e60fe>:298: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(): # Apply autocast context manager for FP16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model exported to ONNX successfully.\n",
            "ONNX Runtime providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "ONNX Runtime session created.\n",
            "Preparing conceptual LivePortrait input for batch size 1 (replace with actual LivePortrait data loading)...\n",
            "Running warm-up for ONNX Runtime model...\n",
            "Starting actual timed run for ONNX Runtime model...\n",
            "\n",
            "ONNX Runtime Inference Time: 0.0432 seconds\n",
            "ONNX Runtime Max GPU Memory Usage (PyTorch perspective): 0.01 GB\n",
            "Note: Actual ONNX Runtime memory usage might differ and may require external tools like `nvidia-smi` to measure.\n",
            "ONNX Runtime Output Shape (conceptual): (1, 1000)\n",
            "Cleaned up liveportrait_optimized.onnx\n",
            "\n",
            "--- Running Optimized Implementation: torch.compile ---\n",
            "Loading conceptual LivePortrait model (replace with actual LivePortrait model loading)...\n",
            "Compiling model with torch.compile...\n",
            "Model compiled.\n",
            "Preparing conceptual LivePortrait input for batch size 1 (replace with actual LivePortrait data loading)...\n",
            "Running warm-up for compiled model...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Starting actual timed run for compiled model...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "\n",
            "torch.compile Inference Time: 0.0005 seconds\n",
            "torch.compile Max GPU Memory Usage: 0.27 GB\n",
            "torch.compile Output Shape (conceptual): torch.Size([1, 1000])\n",
            "\n",
            "--- Consolidated Performance Summary (Fill with Actual Measured Values) ---\n",
            "\n",
            "Metric                    | Original        | FP16            | ONNX Runtime    | torch.compile  \n",
            "--------------------------+-----------------+-----------------+-----------------+----------------\n",
            "Inference Time (s)        | 0.0020          | 0.0025          | 0.0432          | 0.0005         \n",
            "Max GPU Memory (GB)       | 0.04            | 0.03            | 0.01            | 0.27           \n",
            "Output Quality (Visual)   | Baseline        | Similar         | Similar         | Similar        \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n**Note:** The 'N/A' values indicate that the corresponding optimization could not be tested\\ndue to lack of CUDA support or PyTorch version incompatibility. You would replace these\\ncon tus valores medidos reales.\\n\\nSegún los resultados medidos, seleccionarías la optimización o combinación\\nde optimizaciones con mejor rendimiento para tu envío final, y actualizarías la sección de resumen (`liveportrait_optimization_plan`)\\nen consecuencia.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "## LivePortrait Code Optimization Examples\n",
        "\n",
        "This Colab notebook provides runnable examples for the proposed optimization techniques\n",
        "for the LivePortrait model, focusing on reducing inference time and GPU memory usage.\n",
        "\n",
        "**NOTE:** This code uses a *generic* PyTorch model as an example. You will need to\n",
        "adapt the `model` loading, `input_data` preparation, and the actual `inference_function`\n",
        "calls to match the specific implementation details of the LivePortrait GitHub repository.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Environment Setup\n",
        "\n",
        "First, let's set up the Colab environment by cloning the (placeholder) LivePortrait repository\n",
        "and installing necessary dependencies. You'll need to replace `https://github.com/YourLivePortraitRepo/LivePortrait.git`\n",
        "con el URL actual.\n",
        "\n",
        "**IMPORTANT:** Asegúrate de que todos los comandos `!pip install` se ejecuten correctamente en la primera celda\n",
        "de tu notebook de Colab para evitar errores `ModuleNotFoundError`.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# --- Step 1.1: Install necessary Python packages ---\n",
        "# These installations MUST happen before the imports below to ensure modules are available.\n",
        "print(\"--- Installing required Python packages ---\")\n",
        "!pip install numpy torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Or appropriate CUDA version\n",
        "!pip install onnxruntime # THIS IS CRUCIAL FOR THE ONNXRUNTIME ERROR\n",
        "# If LivePortrait has a requirements.txt, you would use:\n",
        "# !pip install -r requirements.txt\n",
        "print(\"--- Package installation complete ---\")\n",
        "\n",
        "\n",
        "# --- Step 1.2: Import necessary libraries ---\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda.amp import autocast # For Mixed Precision (FP16)\n",
        "import onnxruntime # For ONNX model inference\n",
        "import numpy as np\n",
        "import os\n",
        "import gc # For garbage collection and clearing CUDA cache\n",
        "\n",
        "\n",
        "# --- Step 1.3: Placeholder for LivePortrait specific imports and functions ---\n",
        "# In a real scenario, you would replace these with actual LivePortrait imports\n",
        "# from live_portrait.models.model_builder import LivePortraitModel\n",
        "# from live_portrait.utils.io import load_checkpoint\n",
        "# from live_portrait.data.data_loader import get_sample_data\n",
        "# from live_portrait.inference.core import inference_live_portrait\n",
        "\n",
        "\n",
        "# A simple placeholder model for demonstration purposes, mimicking a deep learning model\n",
        "# You will replace this with the actual LivePortrait model definition/loading.\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Linear(256, 1000) # Example output features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Placeholder for LivePortrait's model loading and inference logic\n",
        "# YOU MUST REPLACE THE CONTENT OF THESE FUNCTIONS WITH THE ACTUAL LIVEPORTRAIT CODE\n",
        "def load_liveportrait_model_conceptual():\n",
        "    \"\"\"\n",
        "    Conceptual function to load the LivePortrait model.\n",
        "    In a real scenario, this would load the actual LivePortrait model from a checkpoint.\n",
        "    \"\"\"\n",
        "    print(\"Loading conceptual LivePortrait model (replace with actual LivePortrait model loading)...\")\n",
        "    # --- REPLACE BELOW WITH ACTUAL LIVEPORTRAIT MODEL LOADING LOGIC ---\n",
        "    # Example for LivePortrait:\n",
        "    # from liveportrait.models.builder import build_model\n",
        "    # from liveportrait.utils.checkpoint import load_checkpoint\n",
        "    # config = ... # Load LivePortrait config\n",
        "    # model = build_model(config.model_params)\n",
        "    # load_checkpoint(model, 'path/to/liveportrait_checkpoint.pth') # Ensure this path is correct\n",
        "    # --- END REPLACE ---\n",
        "\n",
        "    # Placeholder model for demonstration (delete this once you have real LivePortrait code)\n",
        "    model = SimpleCNN()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "    model.eval() # Set to evaluation mode for inference\n",
        "    return model\n",
        "\n",
        "def prepare_liveportrait_input_conceptual(batch_size=1):\n",
        "    \"\"\"\n",
        "    Conceptual function to prepare LivePortrait input data.\n",
        "    In a real scenario, this would load and preprocess source image and driving video.\n",
        "    Returns a dummy tensor for demonstration.\n",
        "    \"\"\"\n",
        "    print(f\"Preparing conceptual LivePortrait input for batch size {batch_size} (replace with actual LivePortrait data loading)...\")\n",
        "    # --- REPLACE BELOW WITH ACTUAL LIVEPORTRAIT INPUT PREPARATION LOGIC ---\n",
        "    # Example for LivePortrait:\n",
        "    # source_image_path = 'path/to/your/source_image.jpg'\n",
        "    # driving_video_path = 'path/to/your/driving_video.mp4'\n",
        "    # # Load and preprocess as per LivePortrait's data pipeline\n",
        "    # source_image = ... # Load source image\n",
        "    # driving_frames = ... # Load driving video frames\n",
        "    # preprocessed_inputs = preprocess_for_liveportrait(source_image, driving_frames)\n",
        "    # This might result in multiple tensors or a dictionary of tensors.\n",
        "    # For this example, we'll return a single tensor.\n",
        "    # --- END REPLACE ---\n",
        "\n",
        "    # Placeholder dummy input for demonstration (delete this once you have real LivePortrait code)\n",
        "    dummy_input = torch.randn(batch_size, 3, 256, 256) # Example: Batch, Channels, Height, Width\n",
        "    if torch.cuda.is_available():\n",
        "        dummy_input = dummy_input.cuda()\n",
        "    return dummy_input\n",
        "\n",
        "def run_liveportrait_inference_conceptual(model, input_data):\n",
        "    \"\"\"\n",
        "    Conceptual function to run LivePortrait inference.\n",
        "    In a real scenario, this would invoke the core LivePortrait inference logic.\n",
        "    Returns a dummy output tensor.\n",
        "    \"\"\"\n",
        "    print(\"Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\")\n",
        "    # Disable gradient computation for inference\n",
        "    with torch.no_grad():\n",
        "        # --- REPLACE BELOW WITH ACTUAL LIVEPORTRAIT INFERENCE CALL ---\n",
        "        # Example for LivePortrait:\n",
        "        # result_frames = inference_live_portrait(model, input_data['source'], input_data['driving'])\n",
        "        # --- END REPLACE ---\n",
        "\n",
        "        # Placeholder output for demonstration (delete this once you have real LivePortrait code)\n",
        "        output = model(input_data)\n",
        "    return output\n",
        "\n",
        "def clear_cuda_cache():\n",
        "    \"\"\"Clears CUDA cache to get more accurate memory measurements.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "print(\"\\n--- Initializing LivePortrait Repository (Conceptual) ---\")\n",
        "\n",
        "# Replace with the actual GitHub repository URL\n",
        "liveportrait_repo_url = \"https://github.com/KwaiVGI/LivePortrait.git\" # <--- UPDATED TO THE ACTUAL LIVEPORTRAIT GITHUB URL\n",
        "liveportrait_repo_dir = \"LivePortrait\" # Assuming the cloned directory name\n",
        "\n",
        "# Only clone if not already cloned (for re-running cells)\n",
        "if not os.path.exists(liveportrait_repo_dir):\n",
        "    print(f\"Cloning {liveportrait_repo_url}...\")\n",
        "    # Execute the git clone command\n",
        "    !git clone {liveportrait_repo_url}\n",
        "\n",
        "    # IMPORTANT CHECK: Verify if the directory was created after cloning\n",
        "    if not os.path.exists(liveportrait_repo_dir):\n",
        "        print(f\"\\nERROR: Directory '{liveportrait_repo_dir}' was not created after cloning '{liveportrait_repo_url}'.\")\n",
        "        print(\"Please check the following:\")\n",
        "        print(\"1. Is the 'liveportrait_repo_url' correct and publicly accessible?\")\n",
        "        print(\"2. Does the repository actually clone into a folder named 'LivePortrait'? It might be a different name.\")\n",
        "        print(\"   If it clones into a different name (e.g., 'live_portrait_repo'), update 'liveportrait_repo_dir' accordingly.\")\n",
        "        raise FileNotFoundError(f\"Cloned directory '{liveportrait_repo_dir}' not found. Aborting setup.\")\n",
        "\n",
        "    # Change current directory into the cloned repository\n",
        "    # IMPORTANT: After cloning, all subsequent file paths should be relative to this directory\n",
        "    print(f\"Changing directory to {liveportrait_repo_dir}\")\n",
        "    os.chdir(liveportrait_repo_dir)\n",
        "\n",
        "    # Install specific requirements for LivePortrait if not covered above\n",
        "    if os.path.exists('requirements.txt'):\n",
        "        print(\"Installing LivePortrait specific requirements from requirements.txt...\")\n",
        "        !pip install -r requirements.txt\n",
        "    else:\n",
        "        print(\"No requirements.txt found in LivePortrait repo. Ensure all dependencies are met.\")\n",
        "\n",
        "    # Placeholder for downloading pre-trained models\n",
        "    # YOU WILL LIKELY NEED TO DOWNLOAD LIVEPORTRAIT PRE-TRAINED MODELS HERE\n",
        "    # Check LivePortrait's GitHub README for instructions on downloading models.\n",
        "    # Example:\n",
        "    # if not os.path.exists(\"checkpoints\"):\n",
        "    #     os.makedirs(\"checkpoints\")\n",
        "    #     print(\"Created 'checkpoints' directory.\")\n",
        "    # !wget -O checkpoints/model_name.pth https://example.com/path/to/pretrained/liveportrait_model.pth\n",
        "\n",
        "else:\n",
        "    print(f\"Directory '{liveportrait_repo_dir}' already exists. Skipping cloning.\")\n",
        "    # Ensure we are in the correct directory if rerunning\n",
        "    # This check prevents errors if you've already changed directory in a previous cell and run this cell again\n",
        "    current_dir_name = os.path.basename(os.getcwd())\n",
        "    if current_dir_name != liveportrait_repo_dir:\n",
        "        print(f\"Changing directory to {liveportrait_repo_dir}\")\n",
        "        os.chdir(liveportrait_repo_dir)\n",
        "\n",
        "\n",
        "print(\"\\nEnvironment setup complete (conceptual LivePortrait repo setup).\\n\")\n",
        "\n",
        "\"\"\"\n",
        "### 2. Original Implementation and Baseline Measurement\n",
        "\n",
        "We'll establish a baseline for inference time and GPU memory usage using the original\n",
        "(conceptual) model and inference function.\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- Running Original Implementation (Baseline) ---\")\n",
        "\n",
        "clear_cuda_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# Load the conceptual LivePortrait model\n",
        "model_original = load_liveportrait_model_conceptual()\n",
        "# Prepare a single input for latency measurement\n",
        "input_original = prepare_liveportrait_input_conceptual(batch_size=1)\n",
        "\n",
        "# Warm-up run: The first run can be slower due to CUDA context initialization\n",
        "# Run a few times to get stable measurements\n",
        "print(\"Running warm-up for original model...\")\n",
        "for _ in range(5):\n",
        "    _ = run_liveportrait_inference_conceptual(model_original, input_original)\n",
        "clear_cuda_cache() # Clear cache after warm-up\n",
        "\n",
        "# Actual timed run\n",
        "print(\"Starting actual timed run for original model...\")\n",
        "start_time_original = time.time()\n",
        "# Run inference with the original model\n",
        "output_original = run_liveportrait_inference_conceptual(model_original, input_original)\n",
        "end_time_original = time.time()\n",
        "\n",
        "inference_time_original = end_time_original - start_time_original\n",
        "print(f\"\\nOriginal Inference Time: {inference_time_original:.4f} seconds\")\n",
        "\n",
        "max_memory_original = 0\n",
        "if torch.cuda.is_available():\n",
        "    max_memory_original = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "    print(f\"Original Max GPU Memory Usage: {max_memory_original:.2f} GB\")\n",
        "else:\n",
        "    print(\"CUDA not available, GPU memory usage not measured.\")\n",
        "\n",
        "print(\"Original Output Shape (conceptual):\", output_original.shape)\n",
        "# In a real LivePortrait scenario, you'd save or display the output video/image here.\n",
        "# Example: save_video(output_original, 'results/output_original.mp4')\n",
        "\n",
        "# Clean up original model to free memory before optimizations\n",
        "del model_original\n",
        "del input_original\n",
        "del output_original\n",
        "clear_cuda_cache()\n",
        "\n",
        "\"\"\"\n",
        "### 3. Optimized Implementations\n",
        "\n",
        "Now, let's apply and test each of the proposed optimization techniques.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3.1. Mixed Precision (FP16) Inference\n",
        "\n",
        "This involves using `torch.cuda.amp.autocast` to perform operations in half-precision.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Running Optimized Implementation: Mixed Precision (FP16) ---\")\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Skipping FP16 optimization: CUDA not available.\")\n",
        "    # Placeholder values for summary table if skipped\n",
        "    inference_time_fp16 = \"N/A (CUDA)\"\n",
        "    max_memory_fp16 = \"N/A (CUDA)\"\n",
        "else:\n",
        "    clear_cuda_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    model_fp16 = load_liveportrait_model_conceptual() # Load model again\n",
        "    # Ensure model is on GPU for FP16\n",
        "    model_fp16 = model_fp16.cuda() # Already done in load_... if CUDA available\n",
        "\n",
        "    input_fp16 = prepare_liveportrait_input_conceptual(batch_size=1)\n",
        "    # Ensure input is on GPU for FP16\n",
        "    input_fp16 = input_fp16.cuda() # Already done in prepare_... if CUDA available\n",
        "\n",
        "    # Warm-up run\n",
        "    print(\"Running warm-up for FP16 model...\")\n",
        "    for _ in range(5):\n",
        "        with autocast():\n",
        "            _ = run_liveportrait_inference_conceptual(model_fp16, input_fp16)\n",
        "    clear_cuda_cache() # Clear cache after warm-up\n",
        "\n",
        "    # Actual timed run\n",
        "    print(\"Starting actual timed run for FP16 model...\")\n",
        "    start_time_fp16 = time.time()\n",
        "    with autocast(): # Apply autocast context manager for FP16\n",
        "        output_fp16 = run_liveportrait_inference_conceptual(model_fp16, input_fp16)\n",
        "    end_time_fp16 = time.time()\n",
        "\n",
        "    inference_time_fp16 = end_time_fp16 - start_time_fp16\n",
        "    print(f\"\\nFP16 Inference Time: {inference_time_fp16:.4f} seconds\")\n",
        "\n",
        "    max_memory_fp16 = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "    print(f\"FP16 Max GPU Memory Usage: {max_memory_fp16:.2f} GB\")\n",
        "    print(\"FP16 Output Shape (conceptual):\", output_fp16.shape)\n",
        "\n",
        "    del model_fp16\n",
        "    del input_fp16\n",
        "    del output_fp16\n",
        "    clear_cuda_cache()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "#### 3.2. Model Export to ONNX and Inference with ONNX Runtime\n",
        "\n",
        "This involves exporting the PyTorch model to ONNX format and then running inference\n",
        "using the ONNX Runtime.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Running Optimized Implementation: ONNX Runtime ---\")\n",
        "\n",
        "# Helper function to convert PyTorch tensor to NumPy array\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "onnx_model_path = \"liveportrait_optimized.onnx\"\n",
        "\n",
        "try:\n",
        "    # Load the conceptual LivePortrait model for ONNX export\n",
        "    model_onnx_export = load_liveportrait_model_conceptual()\n",
        "    # Create a dummy input for tracing the model\n",
        "    # IMPORTANT: The dummy input must have the EXACT same shape and dtype as your real input\n",
        "    # for ONNX tracing to be successful.\n",
        "    dummy_input_onnx = prepare_liveportrait_input_conceptual(batch_size=1)\n",
        "\n",
        "    print(f\"Exporting model to ONNX: {onnx_model_path}\")\n",
        "    torch.onnx.export(model_onnx_export,\n",
        "                      dummy_input_onnx,\n",
        "                      onnx_model_path,\n",
        "                      export_params=True,\n",
        "                      opset_version=17, # Recommended opset version for modern PyTorch\n",
        "                      do_constant_folding=True,\n",
        "                      input_names=['input'],\n",
        "                      output_names=['output'],\n",
        "                      dynamic_axes={'input' : {0 : 'batch_size'}} # Allow dynamic batch size for flexible inputs\n",
        "                     )\n",
        "    print(\"Model exported to ONNX successfully.\")\n",
        "\n",
        "    # Determine providers for ONNX Runtime\n",
        "    # Prefer CUDAExecutionProvider if available, otherwise fallback to CPU\n",
        "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']\n",
        "    print(f\"ONNX Runtime providers: {providers}\")\n",
        "\n",
        "    # Load the ONNX model and create an ONNX Runtime session\n",
        "    ort_session = onnxruntime.InferenceSession(onnx_model_path, providers=providers)\n",
        "    print(\"ONNX Runtime session created.\")\n",
        "\n",
        "    clear_cuda_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    # Prepare input for ONNX Runtime (needs to be NumPy array on CPU)\n",
        "    input_onnx_np = to_numpy(prepare_liveportrait_input_conceptual(batch_size=1))\n",
        "\n",
        "    # Warm-up run for ONNX Runtime\n",
        "    print(\"Running warm-up for ONNX Runtime model...\")\n",
        "    for _ in range(5):\n",
        "        ort_inputs = {ort_session.get_inputs()[0].name: input_onnx_np}\n",
        "        _ = ort_session.run(None, ort_inputs)\n",
        "    clear_cuda_cache() # Clear cache after warm-up\n",
        "\n",
        "    # Actual timed run\n",
        "    print(\"Starting actual timed run for ONNX Runtime model...\")\n",
        "    start_time_onnx = time.time()\n",
        "    # Run inference with ONNX Runtime\n",
        "    ort_inputs = {ort_session.get_inputs()[0].name: input_onnx_np}\n",
        "    ort_outs = ort_session.run(None, ort_inputs) # ort_outs is a list of output arrays\n",
        "    output_onnx = ort_outs[0] # Assuming single output\n",
        "    end_time_onnx = time.time()\n",
        "\n",
        "    inference_time_onnx = end_time_onnx - start_time_onnx\n",
        "    print(f\"\\nONNX Runtime Inference Time: {inference_time_onnx:.4f} seconds\")\n",
        "\n",
        "    # ONNX Runtime memory usage might not be directly captured by torch.cuda.max_memory_allocated()\n",
        "    # as it manages its own memory. For a comprehensive comparison, you might need NVIDIA-SMI.\n",
        "    max_memory_onnx = 0\n",
        "    if torch.cuda.is_available():\n",
        "        max_memory_onnx = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "        print(f\"ONNX Runtime Max GPU Memory Usage (PyTorch perspective): {max_memory_onnx:.2f} GB\")\n",
        "        print(\"Note: Actual ONNX Runtime memory usage might differ and may require external tools like `nvidia-smi` to measure.\")\n",
        "\n",
        "    print(\"ONNX Runtime Output Shape (conceptual):\", output_onnx.shape)\n",
        "\n",
        "    del model_onnx_export\n",
        "    del dummy_input_onnx\n",
        "    del ort_session # Delete the session\n",
        "    clear_cuda_cache()\n",
        "    os.remove(onnx_model_path) # Clean up the exported ONNX model file\n",
        "    print(f\"Cleaned up {onnx_model_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ONNX Export/Inference failed: {e}\")\n",
        "    # Ensure cleanup even if error occurs\n",
        "    if os.path.exists(onnx_model_path):\n",
        "        os.remove(onnx_model_path)\n",
        "    # Placeholder values for summary table if skipped\n",
        "    inference_time_onnx = \"N/A (ONNX Fail)\"\n",
        "    max_memory_onnx = \"N/A (ONNX Fail)\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "#### 3.3. JIT Compilation with `torch.compile` (PyTorch 2.0+)\n",
        "\n",
        "This optimization leverages PyTorch 2.0's `torch.compile` feature for graph compilation.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Running Optimized Implementation: torch.compile ---\")\n",
        "\n",
        "# Check PyTorch version\n",
        "if not torch.__version__.startswith(\"2.\"): # Use .startswith(\"2.\") for any 2.x version\n",
        "    print(f\"Skipping torch.compile: Requires PyTorch 2.0+ (current version: {torch.__version__}).\")\n",
        "    inference_time_compiled = \"N/A (PyTorch < 2.0)\"\n",
        "    max_memory_compiled = \"N/A (PyTorch < 2.0)\"\n",
        "elif not torch.cuda.is_available():\n",
        "    print(\"Skipping torch.compile: CUDA not available, torch.compile is most effective on GPU.\")\n",
        "    inference_time_compiled = \"N/A (CUDA)\"\n",
        "    max_memory_compiled = \"N/A (CUDA)\"\n",
        "else:\n",
        "    clear_cuda_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    model_compiled = load_liveportrait_model_conceptual() # Load model again\n",
        "    # Ensure model is on GPU for compilation\n",
        "    model_compiled = model_compiled.cuda()\n",
        "\n",
        "    print(\"Compiling model with torch.compile...\")\n",
        "    # Use \"reduce-overhead\" for faster compilation, \"max-autotune\" for best performance\n",
        "    # For a real project, you might experiment with different modes.\n",
        "    compiled_model = torch.compile(model_compiled, mode=\"reduce-overhead\")\n",
        "    print(\"Model compiled.\")\n",
        "\n",
        "    input_compiled = prepare_liveportrait_input_conceptual(batch_size=1)\n",
        "    # Ensure input is on GPU\n",
        "    input_compiled = input_compiled.cuda()\n",
        "\n",
        "    # Warm-up run for compiled model\n",
        "    print(\"Running warm-up for compiled model...\")\n",
        "    for _ in range(5):\n",
        "        _ = run_liveportrait_inference_conceptual(compiled_model, input_compiled)\n",
        "    clear_cuda_cache() # Clear cache after warm-up\n",
        "\n",
        "    # Actual timed run\n",
        "    print(\"Starting actual timed run for compiled model...\")\n",
        "    start_time_compiled = time.time()\n",
        "    output_compiled = run_liveportrait_inference_conceptual(compiled_model, input_compiled)\n",
        "    end_time_compiled = time.time()\n",
        "\n",
        "    inference_time_compiled = end_time_compiled - start_time_compiled\n",
        "    print(f\"\\ntorch.compile Inference Time: {inference_time_compiled:.4f} seconds\")\n",
        "\n",
        "    max_memory_compiled = torch.cuda.max_memory_allocated() / (1024**3)\n",
        "    print(f\"torch.compile Max GPU Memory Usage: {max_memory_compiled:.2f} GB\")\n",
        "    print(\"torch.compile Output Shape (conceptual):\", output_compiled.shape)\n",
        "\n",
        "    del model_compiled\n",
        "    del compiled_model\n",
        "    del input_compiled\n",
        "    del output_compiled\n",
        "    clear_cuda_cache()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "### 4. Consolidated Performance Summary (Conceptual)\n",
        "\n",
        "This section would present the combined results from the actual runs.\n",
        "You would fill in the table with the *measured* values.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Consolidated Performance Summary (Fill with Actual Measured Values) ---\")\n",
        "\n",
        "# Retrieve actual measured values from the execution above or use N/A if skipped\n",
        "actual_inference_time_original = inference_time_original if 'inference_time_original' in locals() else \"N/A (Error)\"\n",
        "actual_max_memory_original = max_memory_original if 'max_memory_original' in locals() else \"N/A (Error)\"\n",
        "\n",
        "# Use variables defined in the respect"
      ],
      "metadata": {
        "id": "Nvnueye4MNNC",
        "outputId": "bf76b02b-89d8-4036-c110-6e888dd1a151",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Installing required Python packages ---\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (1.22.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (4.25.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "--- Package installation complete ---\n",
            "\n",
            "--- Initializing LivePortrait Repository (Conceptual) ---\n",
            "Cloning https://github.com/KwaiVGI/LivePortrait.git...\n",
            "Cloning into 'LivePortrait'...\n",
            "remote: Enumerating objects: 1071, done.\u001b[K\n",
            "remote: Counting objects: 100% (293/293), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 1071 (delta 261), reused 247 (delta 247), pack-reused 778 (from 3)\u001b[K\n",
            "Receiving objects: 100% (1071/1071), 38.77 MiB | 37.59 MiB/s, done.\n",
            "Resolving deltas: 100% (544/544), done.\n",
            "Changing directory to LivePortrait\n",
            "Installing LivePortrait specific requirements from requirements.txt...\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: pyyaml==6.0.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: opencv-python==4.10.0.84 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 3)) (4.10.0.84)\n",
            "Requirement already satisfied: scipy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: imageio==2.34.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 5)) (2.34.2)\n",
            "Requirement already satisfied: lmdb==1.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: tqdm==4.66.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 7)) (4.66.4)\n",
            "Requirement already satisfied: rich==13.7.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 8)) (13.7.1)\n",
            "Requirement already satisfied: ffmpeg-python==0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 9)) (0.2.0)\n",
            "Requirement already satisfied: onnx==1.16.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 10)) (1.16.1)\n",
            "Requirement already satisfied: scikit-image==0.24.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 11)) (0.24.0)\n",
            "Requirement already satisfied: albumentations==1.4.10 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 12)) (1.4.10)\n",
            "Requirement already satisfied: matplotlib==3.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 13)) (3.9.0)\n",
            "Requirement already satisfied: imageio-ffmpeg==0.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 14)) (0.5.1)\n",
            "Requirement already satisfied: tyro==0.8.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 15)) (0.8.5)\n",
            "Requirement already satisfied: gradio==5.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 16)) (5.1.0)\n",
            "Requirement already satisfied: pykalman==0.9.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 17)) (0.9.7)\n",
            "Requirement already satisfied: pillow>=10.2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements_base.txt (line 18)) (10.4.0)\n",
            "Requirement already satisfied: onnxruntime-gpu==1.18.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.18.0)\n",
            "Requirement already satisfied: transformers==4.38.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (4.38.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich==13.7.1->-r requirements_base.txt (line 8)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich==13.7.1->-r requirements_base.txt (line 8)) (2.18.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python==0.2.0->-r requirements_base.txt (line 9)) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx==1.16.1->-r requirements_base.txt (line 10)) (4.25.6)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.24.0->-r requirements_base.txt (line 11)) (3.4.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.24.0->-r requirements_base.txt (line 11)) (2025.2.18)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.24.0->-r requirements_base.txt (line 11)) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.24.0->-r requirements_base.txt (line 11)) (0.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.10->-r requirements_base.txt (line 12)) (4.12.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.10->-r requirements_base.txt (line 12)) (1.6.1)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.10->-r requirements_base.txt (line 12)) (2.10.6)\n",
            "Requirement already satisfied: albucore>=0.0.11 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.10->-r requirements_base.txt (line 12)) (0.0.23)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.10->-r requirements_base.txt (line 12)) (4.11.0.86)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0->-r requirements_base.txt (line 13)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0->-r requirements_base.txt (line 13)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0->-r requirements_base.txt (line 13)) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0->-r requirements_base.txt (line 13)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0->-r requirements_base.txt (line 13)) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0->-r requirements_base.txt (line 13)) (2.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from imageio-ffmpeg==0.5.1->-r requirements_base.txt (line 14)) (75.1.0)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro==0.8.5->-r requirements_base.txt (line 15)) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro==0.8.5->-r requirements_base.txt (line 15)) (1.7.2)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (1.4.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (3.10.15)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (2.2.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (0.11.12)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (0.15.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r requirements_base.txt (line 16)) (0.34.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.18.0->-r requirements.txt (line 3)) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.18.0->-r requirements.txt (line 3)) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.18.0->-r requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.0->-r requirements.txt (line 4)) (3.17.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.0->-r requirements.txt (line 4)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.0->-r requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.0->-r requirements.txt (line 4)) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.0->-r requirements.txt (line 4)) (0.5.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.4.0->gradio==5.1.0->-r requirements_base.txt (line 16)) (2024.10.0)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.4.0->gradio==5.1.0->-r requirements_base.txt (line 16)) (12.0)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore>=0.0.11->albumentations==1.4.10->-r requirements_base.txt (line 12)) (3.12.2)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore>=0.0.11->albumentations==1.4.10->-r requirements_base.txt (line 12)) (6.2.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio==5.1.0->-r requirements_base.txt (line 16)) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio==5.1.0->-r requirements_base.txt (line 16)) (1.3.1)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1.0->gradio==5.1.0->-r requirements_base.txt (line 16)) (0.46.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==5.1.0->-r requirements_base.txt (line 16)) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==5.1.0->-r requirements_base.txt (line 16)) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==5.1.0->-r requirements_base.txt (line 16)) (0.14.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich==13.7.1->-r requirements_base.txt (line 8)) (0.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==5.1.0->-r requirements_base.txt (line 16)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==5.1.0->-r requirements_base.txt (line 16)) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.0->albumentations==1.4.10->-r requirements_base.txt (line 12)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.0->albumentations==1.4.10->-r requirements_base.txt (line 12)) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.9.0->-r requirements_base.txt (line 13)) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.2->albumentations==1.4.10->-r requirements_base.txt (line 12)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.2->albumentations==1.4.10->-r requirements_base.txt (line 12)) (3.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==5.1.0->-r requirements_base.txt (line 16)) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==5.1.0->-r requirements_base.txt (line 16)) (1.5.4)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime-gpu==1.18.0->-r requirements.txt (line 3)) (10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.0->-r requirements.txt (line 4)) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.0->-r requirements.txt (line 4)) (2.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime-gpu==1.18.0->-r requirements.txt (line 3)) (1.3.0)\n",
            "\n",
            "Environment setup complete (conceptual LivePortrait repo setup).\n",
            "\n",
            "--- Running Original Implementation (Baseline) ---\n",
            "Loading conceptual LivePortrait model (replace with actual LivePortrait model loading)...\n",
            "Preparing conceptual LivePortrait input for batch size 1 (replace with actual LivePortrait data loading)...\n",
            "Running warm-up for original model...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Starting actual timed run for original model...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "\n",
            "Original Inference Time: 0.0020 seconds\n",
            "Original Max GPU Memory Usage: 0.04 GB\n",
            "Original Output Shape (conceptual): torch.Size([1, 1000])\n",
            "\n",
            "--- Running Optimized Implementation: Mixed Precision (FP16) ---\n",
            "Loading conceptual LivePortrait model (replace with actual LivePortrait model loading)...\n",
            "Preparing conceptual LivePortrait input for batch size 1 (replace with actual LivePortrait data loading)...\n",
            "Running warm-up for FP16 model...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Starting actual timed run for FP16 model...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "\n",
            "FP16 Inference Time: 0.0024 seconds\n",
            "FP16 Max GPU Memory Usage: 0.03 GB\n",
            "FP16 Output Shape (conceptual): torch.Size([1, 1000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b4c8196271b0>:293: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "<ipython-input-2-b4c8196271b0>:300: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(): # Apply autocast context manager for FP16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running Optimized Implementation: ONNX Runtime ---\n",
            "Loading conceptual LivePortrait model (replace with actual LivePortrait model loading)...\n",
            "Preparing conceptual LivePortrait input for batch size 1 (replace with actual LivePortrait data loading)...\n",
            "Exporting model to ONNX: liveportrait_optimized.onnx\n",
            "Model exported to ONNX successfully.\n",
            "ONNX Runtime providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "ONNX Runtime session created.\n",
            "Preparing conceptual LivePortrait input for batch size 1 (replace with actual LivePortrait data loading)...\n",
            "Running warm-up for ONNX Runtime model...\n",
            "Starting actual timed run for ONNX Runtime model...\n",
            "\n",
            "ONNX Runtime Inference Time: 0.0399 seconds\n",
            "ONNX Runtime Max GPU Memory Usage (PyTorch perspective): 0.01 GB\n",
            "Note: Actual ONNX Runtime memory usage might differ and may require external tools like `nvidia-smi` to measure.\n",
            "ONNX Runtime Output Shape (conceptual): (1, 1000)\n",
            "Cleaned up liveportrait_optimized.onnx\n",
            "\n",
            "--- Running Optimized Implementation: torch.compile ---\n",
            "Loading conceptual LivePortrait model (replace with actual LivePortrait model loading)...\n",
            "Compiling model with torch.compile...\n",
            "Model compiled.\n",
            "Preparing conceptual LivePortrait input for batch size 1 (replace with actual LivePortrait data loading)...\n",
            "Running warm-up for compiled model...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "Starting actual timed run for compiled model...\n",
            "Running conceptual LivePortrait inference (replace with actual LivePortrait inference call)...\n",
            "\n",
            "torch.compile Inference Time: 0.0007 seconds\n",
            "torch.compile Max GPU Memory Usage: 0.25 GB\n",
            "torch.compile Output Shape (conceptual): torch.Size([1, 1000])\n",
            "\n",
            "--- Consolidated Performance Summary (Fill with Actual Measured Values) ---\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}